{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reidentify known parameters for a simple SIR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Starsim framework includes a built-in susceptible-infected-recovered (SIR) within-host progression module that can be used as a building block to developing more realistic agent-based models. Here, we use that SIR within-host module in combination with mixing pool transmission to create a simple SIR disease model. \n",
    "\n",
    "Here, we use that SIR disease model to test two calibration workflows. The goal is to assess if they able to re-identify a known \"true\" set of parameters and explore the resulting latent trajectories. The two workflows demonstrated are:\n",
    "1. [Likelihood maximization using the optuna package as integrated with Starsim](#optimization)\n",
    "2. [Posterior sampling using a Bayesian workflow](#sampling)\n",
    "\n",
    " Re-identifying the known parameter values from synthetic data provides reassurance that the calibration workflow is functioning as expected. For a more realistic Starsim calibration example, please see [Calibration](workflows_calibration.ipynb).\n",
    "\n",
    "[TODO]: <> (Add link to ModelingHub deterministic example once the URL is live.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure notebook autoreloading and inline plotting\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "##%% Imports and settings\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "import starsim as ss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sciris as sc\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import betaln, gammaln\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=optuna.exceptions.ExperimentalWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIR model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a simple agent-based SIR simulation using Starsim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim():\n",
    "    \"\"\"\n",
    "    Make a simple SIR simulation using Starsim.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sim\n",
    "        A single simulation that has been configured, but not initialized or run.\n",
    "    \"\"\"\n",
    "    sir_pars = dict(\n",
    "        init_prev = 0.01, # Initial prevalence\n",
    "        p_death = 0,      # No deaths\n",
    "    )\n",
    "    sir = ss.SIR(pars=sir_pars)\n",
    "\n",
    "    net = ss.MixingPool(\n",
    "        beta = 1,       # This is a multiplier on the disease beta\n",
    "        n_contacts = 2, # Poisson lambda\n",
    "    )\n",
    "\n",
    "    sim_pars = dict(\n",
    "        n_agents = 1000,      # Number of agents in the simulation\n",
    "        dt = ss.days(1),      # One-day time step\n",
    "        start = '2025-01-01', # Start date of the simulation\n",
    "        dur = 100,            # Duration of the simulation\n",
    "        verbose = 0,          # Level of detail printed to the console\n",
    "    )\n",
    "    sim = ss.Sim(pars=sim_pars, diseases=sir, networks=net)\n",
    "    return sim\n",
    "\n",
    "def modify_sim(sim, calib_pars, rand_seed=0):\n",
    "    \"\"\"\n",
    "    Modify the given simulation with the calibration parameters and random seed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sim : Sim\n",
    "        The simulation to modify.\n",
    "    calib_pars : dict\n",
    "        The calibration parameters to apply; note that the parameter values to use are stored in \"value.\"\n",
    "    rand_seed : int\n",
    "        The random seed to use for the simulation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sim\n",
    "        The modified simulation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Explicitly look for each of the calibration parameters and set the appropriate values\n",
    "    if 'beta' in calib_pars:\n",
    "        β = ss.perday(calib_pars['beta']['value']) # Use per-day for the transmission rate\n",
    "        sim.pars.diseases.pars['beta'] = β\n",
    "\n",
    "    if 'gamma' in calib_pars:\n",
    "        γ = calib_pars['gamma']['value']\n",
    "        sim.pars.diseases.pars['dur_inf'].set(1/γ)\n",
    "\n",
    "    sim.pars['rand_seed'] = rand_seed\n",
    "\n",
    "    return sim\n",
    "\n",
    "def run_starsim(pars, rand_seed=0):\n",
    "    \"\"\"\n",
    "    Run a Starsim SIR model with given parameters and random seed, returning results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pars : dict\n",
    "        The parameters dictionary with Optuna a values stored in \"value.\"\n",
    "    rand_seed : int\n",
    "        The random seed to use for the simulation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        The results of the SIR model with columns of S, I, and R and index of Time.\n",
    "    \"\"\"\n",
    "    # Make and modify the simulation\n",
    "    sim = make_sim()\n",
    "    sim = modify_sim(sim, pars, rand_seed)\n",
    "\n",
    "    sim.run() # Run the simulation\n",
    "\n",
    "    # Extract the results\n",
    "    results = pd.DataFrame(dict(\n",
    "        S = sim.results.sir.n_susceptible,\n",
    "        I = sim.results.sir.n_infected,\n",
    "        R = sim.results.sir.n_recovered,\n",
    "    ), index=pd.Index(sim.results.timevec, name='Time'))\n",
    "    results['rand_seed'] = rand_seed # Store the random seed for later reference\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration setup\n",
    "\n",
    "We will calibrate two parameters, beta and gamma, each allowed to take on values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calibration parameters in a simple dictionary\n",
    "calib_pars = dict(\n",
    "    beta = {'low': 0, 'high': 0.15},\n",
    "    gamma = {'low': 0, 'high': 0.08},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using the optimization-based approach in Starsim, likelihood functions are defined and available for use. However, for the sampling-based approach, we will need to create our own likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_binomial_likelihood(results, calib_data,\n",
    "    kappa       = 10.0,\n",
    "    prior_alpha = 1.0,\n",
    "    prior_beta  = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Likelihood for Beta-Binomial observation model:\n",
    "        s_obs ~ BetaBinomial(n_obs, alpha=kappa*p_hat, beta=kappa*(1-p_hat))\n",
    "        p_hat is computed as (sim_x + prior_alpha) / (sim_n + prior_alpha + prior_beta).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results: DataFrame\n",
    "        The results of a simulation.\n",
    "    calib_data: DataFrame\n",
    "        The calibration data to compare against.\n",
    "    kappa: float\n",
    "        Concentration parameter: larger values imply less over-dispersion.\n",
    "    prior_alpha: float\n",
    "        Prior alpha used to smooth the estimate of p_hat.\n",
    "    prior_beta: float\n",
    "        Prior beta used to smooth the estimate of p_hat.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The likelihood of the observed data given the simulation results.\n",
    "    \"\"\"\n",
    "\n",
    "    obs_x = calib_data['x'].values\n",
    "    obs_n = calib_data['n'].values\n",
    "\n",
    "    sim_x = results['I']\n",
    "    sim_n = results.sum(axis=1)\n",
    "\n",
    "    denom = np.maximum(sim_n + prior_alpha + prior_beta, 1e-12)  # guard\n",
    "    p_hat = (sim_x + prior_alpha) / denom\n",
    "\n",
    "    p_hat = np.clip(p_hat, 1e-9, 1 - 1e-9)\n",
    "    alpha = kappa * p_hat\n",
    "    beta  = kappa * (1.0 - p_hat)\n",
    "\n",
    "    # log binomial coefficient: log C(n, x)\n",
    "    logC = gammaln(obs_n + 1) - gammaln(obs_x + 1) - gammaln(obs_n - obs_x + 1)\n",
    "    loglik = logC + betaln(obs_x + alpha, obs_n - obs_x + beta) - betaln(alpha, beta)\n",
    "    return np.exp(np.sum(loglik)) # Exponentiated sum of logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian calibration requires a prior distribution over the parameters. Here, we define a simple uniform prior over beta and gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_prior(size=1):\n",
    "    \"\"\"\n",
    "    Sample beta and gamma from a uniform prior.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size : int\n",
    "        Number of samples to draw.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "        DataFrame of shape (size, 2) with columns [beta, gamma].\n",
    "    \"\"\"\n",
    "    beta = np.random.uniform(calib_pars['beta']['low'], calib_pars['beta']['high'], size)\n",
    "    gamma = np.random.uniform(calib_pars['gamma']['low'], calib_pars['gamma']['high'], size)\n",
    "\n",
    "    return pd.DataFrame({'beta': beta, 'gamma': gamma})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data for reidentification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That set the basic machinery of the SIR simulation model. Next, we'll create some synthetic data to use as calibration targets. Because the model is stochastic, we'll average over several replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 25 # Average over 25 repetitions to reduce stochastic noise\n",
    "\n",
    "# These are the true parameters the optimizer will later try to identify\n",
    "true_pars = dict(\n",
    "    beta = dict(value=0.05),\n",
    "    gamma = dict(value=0.03),\n",
    ")\n",
    "\n",
    "# Run the starsim SIR simulations in parallel.\n",
    "# If you need to run in serial, for example when debugging, simply set serial=True\n",
    "results_list = sc.parallelize(run_starsim, pars=true_pars, iterkwargs=dict(rand_seed=np.arange(n_reps)), serial=False)\n",
    "results = pd.concat(results_list) # Combine the results into a single DataFrame\n",
    "ave = results.groupby('Time').mean().drop(columns='rand_seed') # Average the results over the repetitions\n",
    "\n",
    "# Extract synthetic data for calibration\n",
    "observation_times = np.array([pd.Timestamp('2025-01-01')+pd.DateOffset(days=d) for d in [20, 40, 80]])\n",
    "starsim_data = pd.DataFrame({\n",
    "    'x': ave.loc[observation_times, 'I'].astype(int),\n",
    "    'n': ave.loc[observation_times].sum(axis=1).astype(int),\n",
    "    'Prevalence': ave.loc[observation_times, 'I'] / ave.loc[observation_times].sum(axis=1)\n",
    "}, index=pd.Index(observation_times, name='t'))\n",
    "\n",
    "print('Here is the data extracted from the average simulation to be used during calibration:\\n')\n",
    "display(starsim_data)\n",
    "\n",
    "# Plot the results, vertical dashed lines indicate the observation times where prevalence is measured\n",
    "df = results.reset_index().melt(id_vars=['Time', 'rand_seed'], value_vars=['S', 'I', 'R'], var_name='State', value_name='Count')\n",
    "ax = sns.lineplot(data=df, hue='State', x='Time', y='Count', units='rand_seed', estimator=None, alpha=0.5, lw=0.5, legend=False)\n",
    "sns.lineplot(data=df, hue='State', x='Time', y='Count', errorbar=('pi', 50), ax=ax, legend=True)\n",
    "ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "for ot in observation_times:\n",
    "    ax.axvline(ot, ls='--', color='black')\n",
    "ax.scatter(starsim_data.index, starsim_data['x'], marker='o', color='black', label='Observed data', zorder=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood maximization using the optuna package as integrated with Starsim\n",
    "<a id=\"optimization\"></a>\n",
    "\n",
    "Starsim provides built-in integration with [Optuna](https://optuna.org/) to make advanced model calibration easier. We demonstrate that linkage below using a Beta-binomial likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = make_sim() # Begin by making a single \"base\" simulation with default parameters\n",
    "\n",
    "# This example will use a single calibration component. We choose a\n",
    "# Beta-binomial functional form to represent the \"prevalence survey\" data,\n",
    "# taking advantage of both the numerator (x) and denominator (n) data.\n",
    "prevalence_component = ss.BetaBinomial(\n",
    "    name = 'SIR Disease Prevalence',\n",
    "\n",
    "    # The starsim_data data has a date for each observation. The\n",
    "    # \"step_containing\" conform method will extract simulation results on the\n",
    "    # time step containing the observation date.\n",
    "    conform = 'step_containing',\n",
    "\n",
    "    # Here is the data we are trying to match, using the \"x\" and \"n\" columns\n",
    "    # from the starsim_data DataFrame.\n",
    "    expected = starsim_data[['x', 'n']],\n",
    "\n",
    "    # And here is how we will extract the relevant data from the simulation results\n",
    "    extract_fn = lambda sim: pd.DataFrame({\n",
    "        'x': sim.results.sir.n_infected, # Numerator\n",
    "        'n': sim.results.n_alive,        # Denominator\n",
    "    }, index=pd.Index(sim.results.timevec, name='t')),\n",
    ")\n",
    "\n",
    "# Now make the calibration\n",
    "calib = ss.Calibration(\n",
    "    sim          = sim,                    # The base simulation\n",
    "    calib_pars   = calib_pars,             # The calibration parameters\n",
    "    build_fn     = modify_sim,             # The function to modify the base simulation with the calibration parameters\n",
    "    reseed       = True,                   # Reseed the simulation for each calibration trial\n",
    "    components   = [prevalence_component], # The calibration components\n",
    "    total_trials = 500,                    # Total number of trials to run\n",
    "    verbose      = False,                  # Shh...\n",
    "\n",
    "    # Select and configure the sampler (optional)\n",
    "    sampler = optuna.samplers.TPESampler(n_startup_trials=50)\n",
    ")\n",
    "\n",
    "calib.calibrate() # Let's go!\n",
    "\n",
    "# Print out a summary\n",
    "sc.colorize(color='blue', string=f'The best parameters identified by the optimization are:\\n\\\n",
    "    * {calib.best_pars}\\n\\\n",
    "    These parameters should be close to the true parameters:\\n\\\n",
    "    * {true_pars}\\n\\\n",
    "    The best parameters resulted in a loss of {calib.study.best_value}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the best parameter values are pretty close to the right values, so that's good. We can look at all the results and easily create a DataFrame containing the top K runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calib.to_df(top_k=10)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "figs = calib.plot_optuna(['plot_optimization_history', 'plot_contour'])\n",
    "figs[0].axes.set_yscale('log')\n",
    "\n",
    "figs[1].axvline(true_pars['beta']['value'], ls='--', color='black')\n",
    "figs[1].axhline(true_pars['gamma']['value'], ls='--', color='black')\n",
    "figs[1].scatter(calib.study.best_params['beta'], calib.study.best_params['gamma'], 100, marker='x', color='red', zorder=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run some simulations with the best parameters and compare to the calibration data. From a mathemtatical perspective, this is not the right thing to do. We have found the maximum-likelihood estimate (MLE) parameters, $\\theta$, and are now running simulations $X \\mid \\theta$. These latent state trajectories represent the intrinsic noise in the model, but do not represent uncertainty. For that, additional methods are required or use a Bayesian approach, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 10 # Number of repetitions to run\n",
    "\n",
    "# These are the best parameters found, the MLE estimate\n",
    "best_pars = dict(\n",
    "    beta = dict(value=calib.study.best_params['beta']),\n",
    "    gamma = dict(value=calib.study.best_params['gamma']),\n",
    ")\n",
    "\n",
    "# Run the starsim SIR simulations in parallel, cool! If you need to run in\n",
    "# serial, for example when debugging, simply set serial=True\n",
    "results_list = sc.parallelize(run_starsim, pars=best_pars, iterkwargs=dict(rand_seed=np.arange(n_reps)), serial=False)\n",
    "results = pd.concat(results_list) # Combine the results into a single DataFrame\n",
    "\n",
    "# Plot the results, vertical dashed lines indicate the observation times where prevalence is measured\n",
    "df = results.reset_index().melt(id_vars=['Time', 'rand_seed'], value_vars=['S', 'I', 'R'], var_name='State', value_name='Count')\n",
    "ax = sns.lineplot(data=df, hue='State', x='Time', y='Count', units='rand_seed', estimator=None, alpha=0.5, lw=0.5, legend=False)\n",
    "sns.lineplot(data=df, hue='State', x='Time', y='Count', errorbar=('pi', 50), ax=ax, legend=True)\n",
    "ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "for ot in observation_times:\n",
    "    ax.axvline(ot, ls='--', color='black')\n",
    "ax.scatter(starsim_data.index, starsim_data['x'], marker='o', color='black', label='Observed data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian calibration using sampling-importance resampling (SIR)\n",
    "<a id=\"sampling\"></a>\n",
    "\n",
    "Now let's use a Bayesian calibration approach to see if the results are any different. We'll use a simple ABC rejection sampling with a beta-binomial likelihood. A key difference is that we're learning a posterior distribution over both parameters and trajectories rather than just a point estimate of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1_000  # Number of prior samples (more is better, but slower)\n",
    "# For Beta-Binomial, the concentration parameter. Lower kappa --> broader\n",
    "# posterior and higher effective sample size (ESS) for the same N. But not a\n",
    "# free parameter, kappa comes from the over-dispersion in the observed data.\n",
    "kappa = 10 \n",
    "\n",
    "# Sample from the prior\n",
    "prior_samples = sample_from_prior(size=N)\n",
    "\n",
    "rand_seeds = np.random.randint(0, 1e6, size=2*N)\n",
    "rand_seeds = np.unique(rand_seeds)[:N]\n",
    "\n",
    "# Prepare parameter dicts for each sample\n",
    "sample_pars_list = [\n",
    "    {\n",
    "        'pars': {'beta': {'value': row['beta']}, 'gamma': {'value': row['gamma']}},\n",
    "        'rand_seed': rand_seeds[idx]  # Random seed for each simulation\n",
    "    }\n",
    "    for idx, row in prior_samples.iterrows()\n",
    "]\n",
    "\n",
    "# Run simulations in parallel and collect trajectories\n",
    "sim_results_list = sc.parallelize(run_starsim, iterkwargs=sample_pars_list, serial=False)\n",
    "\n",
    "# Store latent state trajectories for each sample\n",
    "trajectories = pd.concat(sim_results_list) \\\n",
    "    .reset_index() \\\n",
    "    .set_index(['rand_seed', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute likelihoods for each sample\n",
    "likelihoods = []\n",
    "for rand_seed, sim_result in trajectories.groupby('rand_seed'):\n",
    "    sim_sir = sim_result.loc[rand_seed].loc[observation_times, ['S', 'I', 'R']]\n",
    "    likelihood = beta_binomial_likelihood(sim_sir, starsim_data, kappa=kappa)\n",
    "    likelihoods.append((rand_seed, likelihood))\n",
    "\n",
    "results = pd.DataFrame(likelihoods, columns=['rand_seed', 'likelihood'])\n",
    "results = pd.concat([prior_samples, results], axis=1).set_index('rand_seed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a Bayesian workflow, there are two ways to proceed from this point.\n",
    "1. Use all samples, weighted by their likelihood. For this approach, there is no resampling step.\n",
    "2. Resample $K \\leq N$ samples with probability proportional to their likelihood (with replacement). This is sampling-importance resampling (SIR), not to be confused with the susceptible-infected-recovered (SIR) model. Ideally, we would choose $K=N$ resamples, but if using these samples for further analysis, it may be useful to choose $K < N$ to reduce computational cost. It's generally recommended to choose $K$ as one to two times the effective sample size (ESS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['weight'] = results['likelihood'] / np.sum(results['likelihood'])\n",
    "\n",
    "# Importance resampling\n",
    "ESS = 1 / np.sum(results['weight']**2)\n",
    "print('='*60)\n",
    "print(f'Effective Sample Size (ESS) = {ESS:.1f} out of {N}')\n",
    "if ESS < 30:\n",
    "    print('WARNING: ESS is below 30, consider increasing N')\n",
    "print('='*60)\n",
    "\n",
    "\n",
    "K = np.ceil(1.5 * ESS).astype(int)  # Number of samples to draw\n",
    "print('Resampling K =', K, 'samples from the weighted ensemble of N =', N, 'samples')\n",
    "resample_seeds = np.random.choice(results.index, size=K, replace=True, p=results['weight'])\n",
    "\n",
    "# Merge results into combined (all) and selected (K<N subset)\n",
    "combined = trajectories.reset_index().merge(results, on='rand_seed').set_index(['rand_seed', 'Time'])\n",
    "selected = combined.loc[resample_seeds]\n",
    "\n",
    "# Show posterior samples as a DataFrame\n",
    "posterior_pars = results.loc[resample_seeds, ['beta', 'gamma']]\n",
    "display(posterior_pars.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the parameters, full (weighted) posterior mean, and posterior mean from the resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# Scatter all prior samples, colored by likelihood\n",
    "scat = ax.scatter(\n",
    "    results['beta'], results['gamma'],\n",
    "    c=results['weight'], cmap='viridis', s=15, edgecolor='none', alpha=0.8\n",
    ")\n",
    "\n",
    "# Overlay red rings for the K resampled posterior samples\n",
    "ax.scatter(\n",
    "    results.loc[resample_seeds, 'beta'], results.loc[resample_seeds, 'gamma'],\n",
    "    facecolors='none', edgecolors='red', s=25, linewidths=1, label='Resampled'\n",
    ")\n",
    "\n",
    "# Overlay the true parameter values and best posterior sample\n",
    "ax.axvline(true_pars['beta']['value'], ls='--', color='black', label='True parameters')\n",
    "ax.axhline(true_pars['gamma']['value'], ls='--', color='black')\n",
    "\n",
    "# Mark the full and resampled posterior means\n",
    "full_posterior_mean = np.average(results[['beta', 'gamma']], weights=results['weight'], axis=0)\n",
    "ax.scatter(full_posterior_mean[0], full_posterior_mean[1], 100, marker='x', color='red', lw=2, zorder=10, label='Posterior mean (full)')\n",
    "posterior_mean = results.loc[resample_seeds, ['beta', 'gamma']].mean(axis=0)\n",
    "ax.scatter(posterior_mean[0], posterior_mean[1], 100, marker='x', color='blue', lw=2, zorder=11, label='Posterior mean (resampled)')\n",
    "\n",
    "ax.set_xlabel('beta')\n",
    "ax.set_ylabel('gamma')\n",
    "ax.set_title('Posterior parameter samples (red rings) and prior samples (colored by likelihood)')\n",
    "plt.colorbar(scat, ax=ax, label='Normalized likelihood')\n",
    "# Move the legend outside the figure to the right\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View latent trajectories based on _all_ $N$ samples, weighted by their likelihood. Takes some code to incorporate the weights into the plotting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ALL samples, with weights, to compute mean and quantiles\n",
    "df = combined \\\n",
    "    .reset_index() \\\n",
    "    .melt(\n",
    "        id_vars=['Time', 'rand_seed'],\n",
    "        value_vars=['S', 'I', 'R'],\n",
    "        var_name='State',\n",
    "        value_name='Count'\n",
    "    )\n",
    "df['Weight'] = df['rand_seed'].map(results['weight']) # Add weights to the DataFrame\n",
    "\n",
    "def weighted_quantile(values, quantiles, weights):\n",
    "    v = np.asarray(values, float)\n",
    "    q = np.atleast_1d(quantiles).astype(float)\n",
    "    w = np.asarray(weights, float)\n",
    "    order = np.argsort(v)\n",
    "    v, w = v[order], w[order]\n",
    "    cw = np.cumsum(w)\n",
    "    cw /= cw[-1] if cw[-1] > 0 else 1.0\n",
    "    return np.interp(q, cw, v)\n",
    "\n",
    "def summarize(group):\n",
    "    vals = group['Count'].to_numpy()\n",
    "    wts  = group['Weight'].to_numpy()\n",
    "    mean = np.average(vals, weights=wts)\n",
    "    q05, q25, q75, q95 = weighted_quantile(vals, [0.05, 0.25, 0.75, 0.95], wts)\n",
    "    return pd.Series({'mean': mean, 'q05': q05, 'q25': q25, 'q75': q75, 'q95': q95})\n",
    "\n",
    "summary = (\n",
    "    df.groupby(['Time','State'], sort=True, as_index=False)\n",
    "      .apply(summarize)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "lines = sns.lineplot(data=summary, x='Time', y='mean', hue='State', hue_order=['S', 'I', 'R'],\n",
    "                     estimator=None, errorbar=None, ax=ax, zorder=5)\n",
    "\n",
    "# Get the colors used by seaborn for each state\n",
    "state_colors = {line.get_label(): line.get_color() for line in ax.lines}\n",
    "\n",
    "for state, g in summary.groupby('State'):\n",
    "    color = state_colors.get(state, None)\n",
    "    ax.fill_between(g['Time'], g['q05'], g['q95'], alpha=0.12, linewidth=0, color=color)  # 90% band\n",
    "    ax.fill_between(g['Time'], g['q25'], g['q75'], alpha=0.25, linewidth=0, color=color)  # 50% band\n",
    "\n",
    "for ot in observation_times:\n",
    "    ax.axvline(ot, ls='--', color='black')\n",
    "ax.scatter(starsim_data.index, starsim_data['x'], marker='o', color='black', label='Observed data', zorder=10);\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Weighted posterior trajectories')\n",
    "ax.legend(title='State')\n",
    "ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resampled trajectories. Should be similar to the weighted posterior over latent trajectories shown above.\n",
    "df = selected \\\n",
    "    .reset_index() \\\n",
    "    .melt(\n",
    "        id_vars=['Time', 'rand_seed'],\n",
    "        value_vars=['S', 'I', 'R'],\n",
    "        var_name='State',\n",
    "        value_name='Count'\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "sns.lineplot(data=df, hue='State', x='Time', y='Count', units='rand_seed', estimator=None, alpha=0.5, lw=0.2, legend=False, ax=ax)\n",
    "sns.lineplot(data=df, hue='State', x='Time', y='Count', errorbar=('pi', 50), alpha=0.5, ax=ax, legend=True, zorder=5)\n",
    "sns.lineplot(data=df, hue='State', x='Time', y='Count', errorbar=('pi', 90), alpha=0.25, ax=ax, legend=False, zorder=6)\n",
    "ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "for ot in observation_times:\n",
    "    ax.axvline(ot, ls='--', color='black')\n",
    "ax.scatter(starsim_data.index, starsim_data['x'], marker='o', color='black', label='Observed data', zorder=10);\n",
    "ax.legend(title='State')\n",
    "ax.set_title('Resampled posterior trajectories')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be scenario analysis using these $K$ resamples parameters and latent trajectories. Each of these $K$ samples represents a different possible future trajectory of the epidemic, and the ensemble of these $K$ trajectories can be used to quantify uncertainty in future projections. Each of these $K$ trajectories should be simulated forward $M$ times for each scenario, using common random numbers, to produce final results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
