{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Managing samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "As STIsim models are usually stochastic, for a single scenario it is often desirable to run the model multiple times with different random seeds. The role of the `Samples` class is to facilitate working with large numbers of simulations and scenarios, to ease:\n",
    "\n",
    "- Loading large result sets\n",
    "- Filtering/selecting simulation runs\n",
    "- Plotting individual simulations and aggregate results\n",
    "- Slicing result sets to compare scenarios\n",
    "\n",
    "Essentially, if we think of the processed results of a model run as being\n",
    "\n",
    "- A collection of scalar outputs (e.g., cumulative infections, total deaths)\n",
    "- A dataframe of time-varying outputs (e.g., new diagnoses per day, number of people on treatment each day)\n",
    "\n",
    "then the classes `Dataset` and `Samples` manage collections of these results. In particular, the `Samples` class manages different random samples of the same parameters, and the `Dataset` class manages a collection of `Samples`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    "These classes are particularly designed to facilitate working with tens of thousands of simulation runs, where other approaches such as those based on the `MultiSim` class may not be feasible.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import starsim as ss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sciris as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Obtaining simulation output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "To demonstrate usage of this class, we will first consider constructing the kinds of output that the `Samples` class stores. We begin by running a basic simulation using the SIR model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = ss.People(5000)\n",
    "net = ss.ndict(ss.RandomNet(n_contacts=ss.poisson(5)))\n",
    "sir = ss.SIR()\n",
    "sim = ss.Sim(people=ppl, networks=net, diseases=sir, rand_seed=0)\n",
    "sim.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Dataframe output\n",
    "\n",
    "A `Sim` instance is (in general) too large and complex to efficiently store on disk - the file size and loading time make it prohibitive to work with tens of thousands of simulations. Therefore, rather than storing entire `Sim` instances, we instead store dataframes containing just the simulation results and any other pre-processed calculated quantities. There are broadly speaking two types of outputs\n",
    "\n",
    "- Scalar outputs at each timepoint (e.g., daily new cases)\n",
    "- Scalar outputs for each simulation (e.g., total number of deaths)\n",
    "\n",
    "These outputs can each be produced from a `Sim` - the former has a tabular structure, and the latter has a dictionary structure (which can later be assembled into a table where the rows correspond to each simulation). The `export_df` method is a quick way to obtain a dataframe with the appropriate structure retaining all results from the `Sim`.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    "In real-world use, it is often helpful to write your own function to extract a dataframe of simulation outputs, because typically some of the outputs need to be extracted from custom Analyzers.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Scalar/summary outputs\n",
    "\n",
    "We can also consider extracting a summary dictionary of scalar values. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {}\n",
    "summary['seed'] = sim.pars['rand_seed']\n",
    "summary['p_death'] = sim.diseases[0].pars.p_death.pars.p\n",
    "summary['cum_infections'] = sum(sim.results.sir.new_infections)\n",
    "summary['cum_deaths'] = sum(sim.results.new_deaths)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning alert-info\">    \n",
    "Notice how in the example above, the summary contains both simulation inputs (seed, probability of death) as well as simulation outputs (total infections, total deaths). The simulation summary should contain sufficient information about the simulation inputs to identify the simulation. The seed should generally be present. The other inputs normally correspond to variables that scenarios are being run over. In this example, we will run scenarios comparing simulations with different probabilities of death. Therefore, we need to include the death probability in the simulation summary. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "\n",
    "For usage at scale, the steps of creating a simulation, running it and producing these outputs are usually encapsulated in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(seed, p_death):\n",
    "    ppl = ss.People(5000)\n",
    "    net = ss.RandomNet(n_contacts=ss.poisson(5))\n",
    "    sir = ss.SIR(p_death=p_death)\n",
    "    sim = ss.Sim(people=ppl, networks=net, diseases=sir, rand_seed=seed)\n",
    "    sim.init(verbose=0)\n",
    "    return sim\n",
    "    \n",
    "def run_sim(seed, p_death):\n",
    "    sim = get_sim(seed, p_death)\n",
    "    sim.run(verbose=0)\n",
    "    df = sim.to_df()\n",
    "    \n",
    "    summary = {}\n",
    "    summary['seed'] = sim.pars['rand_seed']\n",
    "    summary['p_death']= sim.diseases[0].pars.p_death.pars.p\n",
    "    summary['cum_infections'] = sum(sim.results.sir.new_infections)\n",
    "    summary['cum_deaths'] = sum(sim.results.new_deaths)\n",
    "    \n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">    \n",
    "The functions above could be combined into a single function. However, in real world usage it is often convenient to be able to construct a simulation independently of running it (e.g., for diagnostic purposes or to allow running the sim in a range of different ways). The suggested structure above, with a <code>get_sim()</code> function and a <code>run_sim()</code> function are recommended as standard practice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Now running a simulation for a given beta/seed value and returning the processed outputs can be done in a single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar output\n",
    "df, summary = run_sim(0, 0.2);\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We can produce all of the samples associated with a scenario by iterating over the input seed values. This is being done in a basic loop here, but could be done in more sophistical ways to leverage parallel computing (e.g., with `sc.parallelize` for single host parallelization, or with `celery` for distributed computation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a collection of sims\n",
    "n = 20\n",
    "seeds = np.arange(n)\n",
    "outputs = [run_sim(seed, 0.2) for seed in seeds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Saving and loading the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We have now produced simulation outputs (dataframes and summary statistics) for 20 simulation runs. The `outputs` here are a list of tuples, containing the dataframe and dictionary outputs for each sample. This list can be passed to the `cvv.Samples` class to produce a single compressed file on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdir = Path('results')\n",
    "resultsdir.mkdir(exist_ok=True, parents=True)\n",
    "ss.Samples.new(resultsdir, outputs, identifiers=[\"p_death\"])\n",
    "list(resultsdir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Notice that a list of `identifiers` should be passed to the `Samples` constructor. This is a list of keys in the simulation summary dictionaries that identifies the scenario. These would be model inputs rather than model outputs, and they should be the same for all of the outputs passed into the `Samples` object. If no file name is explicitly provided, the file will automatically be assigned a name based on the identifiers.\n",
    "\n",
    "<div class=\"alert alert-success\">    \n",
    "The <code>Samples</code> file internally contains metadata recording the identifiers. When <code>Samples</code> are accessed using the <code>Dataset</code> class, they can be accessed via the internal metadata. Therefore for a typical workflow, the file name largely doesn't matter, and it usually doesn't need to be manually specified.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The saved file can be loaded and accessed via the `Samples` class. **Importantly, individual files can be extracted from a `.zip` file without decompressing the entire archive**. This means that loading the summary dataframe and using it to selectively load the full outputs for individual runs can be done efficiently. For example, loading retrieving a single result from a `Samples` file would take a similar amount of time regardless of whether the file contained 10 samples or 100000 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the samples\n",
    "res = ss.Samples('results/0.2.zip')\n",
    "res.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "When the `Samples` file was created, a dictionary of scalars was provided for each result. These are automatically used to populate a 'summary' dataframe, where each identifier (and the seed) are used as the index, and the remaining keys appear as columns, as shown above. As a shortcut, columns of the summary dataframe can be accessed by indexing the `Samples` object directly, without having to access the `.summary` attribute e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['cum_infections']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Each simulation is uniquely identified by its seed, and the time series dataframe for each simulation can be accessed by indexing the `Samples` object with the seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "The dataframes in the `Samples` object are cached, so that the dataframes don't all need to be loaded in order to start working with the file. The first time a dataframe is accessed, it will be loaded from disk. Subsequent requests for the dataframe will return a cached version instead. The cached dataframe is copied each time it is retrieved, to prevent accidentally modifying the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Common analysis operations\n",
    "\n",
    "Here are some examples of common analyses that can be performed using functionality in the `Samples` class\n",
    "\n",
    "### Plotting summary quantities\n",
    "\n",
    "Often it's useful to be able plot distributions of summary quantities, such as the total infections. This can be performed by directly indexing the `Samples` object and then using the appropriate plotting command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res['cum_infections'], density=True)\n",
    "\n",
    "plt.xlabel('Total infections')\n",
    "plt.ylabel('Probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Plotting time series\n",
    "\n",
    "Time series plots can be obtained by accessing the dataframes associated with each seed, and then plotting quantities from those. For convenience, iterating over the `Samples` object will automatically iterate over all of the dataframes associated with each seed. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in res:\n",
    "    plt.plot(df['sir.new_infections'], color='b', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Other ways to access content\n",
    "\n",
    "We have seen so far that we can use\n",
    "\n",
    "- `res.summary` - retrieve dataframe of summary outputs\n",
    "- `res[summary_column]` - retrieve a column of the summary dataframe\n",
    "- `res[seed]` - retrieve the time series dataframe associated with one of the simulations\n",
    "- `for df in res` - iterate over time series dataframes\n",
    "\n",
    "Sometimes it is useful to have access to both the summary dictionary and the time series dataframe associated with a single sample. These can be accessed using the `get` method, which takes in a seed, and returns both outputs for that seed together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.get(0) # Retrieve both summary quantities and dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "In the same way that it is possible to index the `Samples` object directly in order to retrieve columns from the summary dataframe, it is also possible to directly index the `Samples` object to get a column of the time series dataframe. In this case, pass a tuple of items to the `Samples` object, where the first item is the seed, and the second is a column from the time series dataframe. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0,'sir.n_infected'] # Equivalent to `res[0]['sir.n_infected']`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Filtering results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "The `.seeds` attribute contains a listing of seeds, which can be helpful for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "The seeds are drawn from the summary dataframe, which defines which seeds are accessible via the `Samples` object. Therefore, you can drop rows from the summary dataframe to filter the results. For example, suppose we only wanted to analyze simulations with over 4900 deaths. We could retrieve a copy of the summary dataframe that only contains matching simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary.loc[res['cum_infections']>4900]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "We can then make a copy of the results and write the reduced summary dataframe back to that object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = res.copy()\n",
    "res2.summary = res.summary.loc[res['cum_infections']>4900]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">    \n",
    "Unlike <code>sc.dcp()</code>, copying using the <code>.copy()</code> method only deep copies the summary dataframe. It does not duplicate the time series dataframes or the cache. For <code>Samples</code> objects, it is therefore generally preferable to use <code>.copy()</code>.\n",
    "</div>\n",
    "\n",
    "\n",
    "Now notice that there are fewer samples, and the seeds have been filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res2['cum_infections'], density=True)\n",
    "plt.xlabel('Total infections')\n",
    "plt.ylabel('Probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Applying functions and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Sometimes it might be necessary to calculate quantities that are derived from the time series dataframes. These could be simple scalar values, such as totals or averages that had not been computed ahead of time, or extracting values from each simulation at a particular point in time. As an alternative to writing a loop that iterates over the seeds, the `.apply()` method takes in a function and maps it to every dataframe. This makes it quick to construct lists or arrays with scalar values extracted from the time series. For example, suppose we wanted to extract the peak number of people infected from each simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_infections = lambda df: df['sir.n_infected'].max()\n",
    "res.apply(peak_infections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Options when loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "There are two options available when loading that can change how the `Samples` class interacts with the file on disk:\n",
    "\n",
    "- `memory_buffer` - copy the entire file into memory. This prevents the file from being locked on disk and allows scripts to be re-run and results regenerated while still running the analysis notebook. This defaults to `True` for convenience, but loading the entire file into memory can be problematic if the file is large (e.g., >1GB) in which case setting `memory_buffer=False` may be preferable\n",
    "- `preload` - Populate the cache in one step. This facilitates interactive usage of the analysis notebook by making the runtime of analysis functions predictable (since all results will be retrieved from the cache) at the expense of a long initial load time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Implementation details\n",
    "\n",
    "If the file is loaded from a memory buffer, the `._zipfile` attribute will be populated. A helper property `.zipfile` is used to access the buffer, so if caching is not used, `.zipfile` returns the actual file on disk rather than the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ss.Samples('results/0.2.zip', memory_buffer=True) # Copy the entire file into memory\n",
    "print(res._zipfile)\n",
    "print(res.zipfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ss.Samples('results/0.2.zip', memory_buffer=False) # Copy the entire file into memory\n",
    "print(res._zipfile)\n",
    "print(res.zipfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "The dataframes associated with the individual dataframes are cached on access, so `pd.read_csv()` only needs to be called once. The cache starts out empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "res._cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "When a dataframe is accessed, it is automatically stored in the cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]\n",
    "res._cache.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "This means that iterating through the dataframes the first time can be slow (but in general, iterating over all dataframes is avoided in favour of either only using summary outputs, or accessing a subset of the runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.Timer():\n",
    "    for df in res:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.Timer():\n",
    "    for df in res:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "The `preload` option populates the entire cache in advance. This makes creating the `Samples` object slower, but operating on the dataframes afterwards will be consistently fast. This type of usage can be useful when wanting to load large files in the background and then interactively work with them afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.Timer():\n",
    "    res = ss.Samples('results/0.2.zip', preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.Timer():\n",
    "    for df in res:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.Timer():\n",
    "    for df in res:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "Together, these options provide some flexibility in terms of memory and time demands to suit analyses at various different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Running scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "Suppose we wanted to compare a range of different `p_death` values and `initial` values (initial number of infections). We might define these runs as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "initials = np.arange(1,4)\n",
    "p_deaths = np.arange(0,1,0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Recall that our `run_sim()` function had an argument for `p_death`. We can extend this to include the `initial` parameter too. We can actually generalize this further by passing the parameters as keyword arguments to avoid needing to hard-code all of them. Note that we also need to add the `initial` value to the summary outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(seed, **kwargs):\n",
    "    ppl = ss.People(5000)\n",
    "    net = ss.RandomNet(n_contacts=ss.poisson(5))\n",
    "    sir = ss.SIR(pars=kwargs)\n",
    "    sim = ss.Sim(people=ppl, networks=net, diseases=sir, rand_seed=seed)\n",
    "    sim.init(verbose=0)\n",
    "    return sim\n",
    "    \n",
    "def run_sim(seed, **kwargs):\n",
    "    sim = get_sim(seed, **kwargs)\n",
    "    sim.run(verbose=0)\n",
    "    df = sim.to_df()\n",
    "    \n",
    "    summary = {}\n",
    "    summary['seed'] = sim.pars['rand_seed']\n",
    "    summary['p_death']= sim.diseases[0].pars.p_death.pars.p\n",
    "    summary['initial']= sim.diseases[0].pars.init_prev\n",
    "    summary['cum_infections'] = sum(sim.results.sir.new_infections)\n",
    "    summary['cum_deaths'] = sum(sim.results.new_deaths)\n",
    "    \n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "We can now easily run a set of scenarios with different values of `p_death` and save each one to a separate `Samples` object. Note that when we create the `Samples` objects now, we also want to specify that `'initial'` is one of the identifiers for the scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the existing results\n",
    "for file_path in resultsdir.glob('*'):\n",
    "    file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sweep over initial and p_death\n",
    "n = 20\n",
    "seeds = np.arange(n)\n",
    "for init_prev in initials:\n",
    "    for p_death in p_deaths:\n",
    "        outputs = [run_sim(seed, init_prev=init_prev, p_death=p_death) for seed in seeds]\n",
    "        ss.Samples.new(resultsdir, outputs, [\"p_death\", \"init_prev\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "The results folder now contains a collection of saved `Samples` objects. Notice how the automatically selected file names now contain both the `p_death` value and the `initial` value, because they were both specified as identifiers. We can load one of these objects in to see how these identifiers are stored and accessed inside the `Samples` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(resultsdir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ss.Samples('results/0.25-2.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "The 'id' of a `Samples` object is a dictionary of the identifiers, which makes it easy to access the input parameters associated with a set of scenario runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "The 'identifier' is a tuple of these values, which is suitable for use as a dictionary key. This can be useful for accumulating and comparing variables across scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "### Loading multiple scenarios\n",
    "\n",
    "We saw above that we now have a directory full of `.zip` files corresponding to the various scenario runs. These can be accessed using the `Dataset` class, which facilitates accessing multiple instances of `Samples`. We can pass the folder containing the results to the `Dataset` constructor to load them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ss.Dataset(resultsdir)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "The `.ids` attribute lists all of the values available across scenarios in the results folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "The individual results can be accessed by indexing the `Dataset` instance using the values of the identifiers. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0.25,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "This indexing operation is sensitive to the order in which the identifiers are specified. The `.get()` method allows you to specify them as key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get(initial=2, p_death=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "Iterating over the `Dataset` will iterate over the `Samples` instances contained within it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "This can be used to extract and compare values across scenarios. For example, we could consider the use case of making a plot that compares total deaths across scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "y = []\n",
    "yerr = []\n",
    "\n",
    "for res in results:\n",
    "    labels.append(res.id)\n",
    "    y.append(res['cum_deaths'].median())\n",
    "\n",
    "plt.barh(np.arange(len(results)),y, tick_label=labels)\n",
    "plt.xlabel('Median total deaths');\n",
    "plt.ylabel('Scenario')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "### Filtering scenarios\n",
    "\n",
    "Often plots need to be generated for a subset of scenarios e.g., for sensitivity analysis or to otherwise compare specific scenarios. `Dataset.filter` returns a new `Dataset` containing a subset of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results.filter(initial=2):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results.filter(p_death=0.25):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "This is also a quick and efficient operation, so you can easily embed filtering commands inside the analysis to select subsets of the scenarios for plotting and other output generation. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res, color in zip(results.filter(initial=2), sc.gridcolors(4)):\n",
    "    plt.plot(res[0].index, np.median([df['new_deaths'] for df in res], axis=0), color=color, label=f'p_death = {res.id[\"p_death\"]}')\n",
    "plt.legend()\n",
    "plt.title('Sensitivity to p_death (initial = 2)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('New deaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res, color in zip(results.filter(p_death=0.25), sc.gridcolors(3)):\n",
    "    plt.plot(res[0].index, np.median([df['new_deaths'] for df in res], axis=0), color=color, label=f'initial = {res.id[\"initial\"]}')\n",
    "plt.legend()\n",
    "plt.title('Sensitivity to initial infections (p_death = 0.25)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('New deaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "401.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
