{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T7 - Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "An interactive version of this notebook is available on [Google Colab](https://colab.research.google.com/github/starsimhub/starsim/blob/main/docs/tutorials/tut_calibration.ipynb?install=starsim) or [Binder](https://mybinder.org/v2/gh/starsimhub/starsim/HEAD?labpath=docs%2Ftutorials%2Ftut_calibration.ipynb).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disease models typically require contextualization to a relevant setting of interest prior to addressing \"what-if\" scenario questions. The process of tuning model input parameters so that model outputs match observed data is known as calibration. There are many approaches to model calibration, ranging from manual tuning to fully Bayesian methods.\n",
    "\n",
    "For many applications, we have found that an optimization-based approach is sufficient. Such methods avoid the tedious process of manual tuning and are less computationally expensive than fully Bayesian methods. One such optimization-based approach is the Optuna library, which is a Bayesian hyperparameter optimization framework. Optuna is designed for tuning hyperparameters of machine learning models, but it can also be used to calibrate disease models.\n",
    "\n",
    "Calibration libraries often treat the disease model as a black box, where the input parameters are the \"hyperparameters\" to be tuned. The calibration process is often iterative and requires a combination of expert knowledge and computational tools. The optimization algorithm iteratively chooses new parameter values to evaluate, and the model is run with these values to generate outputs. The outputs are compared to observed data, and a loss function is calculated to quantify the difference between the model outputs and the observed data. The optimization algorithm then uses this loss function to update its search strategy and choose new parameter values to evaluate. This process continues until the algorithm converges to a set of parameter values that minimize the loss function.\n",
    "\n",
    "While many optimization algorithms are available, Starsim has a built-in interface to the Optuna library, which we will demonstrate in this tutorial. We will use a simple Susceptible-Infected-Recovered (SIR) model as an example. We will tune three input parameters, the infectivity parameter, `beta`, the initial prevalence parameter, `init_prev`, and the Poisson-distributed degree distribution parameter, `n_contacts`. We will calibrate the model using a beta-binomial likelihood function so as to match prevalence at three distinct time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a few imports and default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports and settings\n",
    "import sciris as sc\n",
    "import starsim as ss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "n_agents = 2e3\n",
    "debug = False # If true, will run in serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration class will require a base `Sim` object. This `sim` will later be modified according to parameters selected by the optimization engine. The following function creates the base `Sim` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim():\n",
    "    sir = ss.SIR(\n",
    "        beta = ss.beta(0.075),\n",
    "        init_prev = ss.bernoulli(0.02),\n",
    "    )\n",
    "    random = ss.RandomNet(n_contacts=ss.poisson(4))\n",
    "\n",
    "    sim = ss.Sim(\n",
    "        n_agents = n_agents,\n",
    "        start = sc.date('2020-01-01'),\n",
    "        stop = sc.date('2020-02-12'),\n",
    "        dt = 1,\n",
    "        unit = 'day',\n",
    "        diseases = sir,\n",
    "        networks = random,\n",
    "        verbose = 0,\n",
    "    )\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the calibration parameters. These are the inputs that Optuna will be able to modify. Here, we define three such parameters, `beta`, `init_prev`, and `n_contacts`.\n",
    "\n",
    "Each parameter entry should have range defined by `low` and `high` as well as a `guess` values. The `guess` value is not used by Optuna, rather only for a check after calibration completes to see if the new parameters are better than the `guess` values.\n",
    "\n",
    "You'll notice there are a few other parameters that can be specified. For example, the data type of the parameter appears in `suggest_type`. Possible values are listed in the Optuna documentation, and include [suggest_float](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float) for float values and [suggest_int](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_int) for integer types.\n",
    "\n",
    "To make things easier for the search algorithm, it's helpful to indicate how outputs are expected to change with inputs. For example, increasing `beta` from 0.01 to 0.02 should double disease transmission, but increasing from 0.11 to 0.12 will have a small effect. Thus, we indicate that this parameter should be calibrated with `log=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calibration parameters\n",
    "calib_pars = dict(\n",
    "    beta = dict(low=0.01, high=0.30, guess=0.15, suggest_type='suggest_float', log=True), # Note the log scale\n",
    "    init_prev = dict(low=0.01, high=0.05, guess=0.15), # Default type is suggest_float, no need to re-specify\n",
    "    n_contacts = dict(low=2, high=10, guess=3, suggest_type='suggest_int'), # Suggest int just for this demo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization engine iteratively chooses input parameters to simulate. Those parameters are passed into the following `build_sim` function as a dictionary of `calib_pars` along with the base `sim` and any other key word arguments. The `calib_pars` will be as above, but importantly will have an additional key named `value` containing the value selected by Optuna.\n",
    "\n",
    "When modifying a `sim`, it is important to realize that the simulation has not been initialized yet. Nonetheless, the configuration is available for modification at `sim.pars`, as demonstrated in the function below for the SIR example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sim(sim, calib_pars, n_reps=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Modify the base simulation by applying calib_pars. The result can be a\n",
    "    single simulation or multiple simulations if n_reps>1. Note that here we are\n",
    "    simply building the simulation by modifying the base sim. Running the sims\n",
    "    and extracting results will be done by the calibration function.\n",
    "    \"\"\"\n",
    "\n",
    "    sir = sim.pars.diseases # There is only one disease in this simulation and it is a SIR\n",
    "    net = sim.pars.networks # There is only one network in this simulation and it is a RandomNet\n",
    "\n",
    "    for k, pars in calib_pars.items(): # Loop over the calibration parameters\n",
    "        if k == 'rand_seed':\n",
    "            sim.pars.rand_seed = v\n",
    "            continue\n",
    "\n",
    "        # Each item in calib_pars is a dictionary with keys like 'low', 'high',\n",
    "        # 'guess', 'suggest_type', and importantly 'value'. The 'value' key is\n",
    "        # the one we want to use as that's the one selected by the algorithm\n",
    "        v = pars['value']\n",
    "        if k == 'beta':\n",
    "            sir.pars.beta = ss.beta(v)\n",
    "        elif k == 'init_prev':\n",
    "            sir.pars.init_prev = ss.bernoulli(v)\n",
    "        elif k == 'n_contacts':\n",
    "            net.pars.n_contacts = ss.poisson(v)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Parameter {k} not recognized')\n",
    "\n",
    "    # If just one simulation per parameter set, return the single simulation\n",
    "    if n_reps == 1:\n",
    "        return sim\n",
    "\n",
    "    # But if you'd like to run multiple simulations with the same parameters, we return a MultiSim instead\n",
    "    # Note that each simulation will have a different random seed, you can set specific seeds if you like\n",
    "    # Also note that parallel=False and debug=True are important to avoid issues with parallelism in the calibration\n",
    "    # Advanced: If running multiple reps, you can choose if/how they are combined using the \"combine_reps\" argument to each CalibComponent, introduced below.\n",
    "    ms = ss.MultiSim(sim, iterpars=dict(rand_seed=np.random.randint(0, 1e6, n_reps)), initialize=True, debug=True, parallel=False)\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Starsim framework has been integrated with the Optuna hyperparameter optimization algorithm to facilitate calibration through the `Calibration` class. Recall that an optimization-based approach to calibration minimizes a function of the input parameters. This function is key to achieving an acceptable calibration.\n",
    "\n",
    "There are two ways to describe the goodness-of-fit function for the `Calibration`. The first method is to directly provide a function that the algorithm will call. The `eval_fn` will be passed each completed `sim` after running, and is expected to return a float representing the **mismatch (lower is better as the optimization algorithm is configured to minimize)**. Data can be passed into the `eval_fn` via `eval_kwargs`.\n",
    "\n",
    "As an alternative to directly specifying the evaluation function, you can use `CalibComponent`s. Each component includes real data, for example from a survey, that is compared against simulation data from the model. Several components and be used at the same time, for example one for disease prevalence and another for treatment coverage. Each component computes a likelihood of the data given the input parameters, as assessed via simulation. Components are combined assuming independence.\n",
    "\n",
    "The base class for a component is called `CalibComponent`, which you can use to define your own likelihood. However, we have provided components for several key likelihood functions including `BetaBinomial`, `Binomial`, `DirichletMultinomial`, `GammaPoisson`, and `Normal`. The `Normal` component is most like a traditional squared error. Each component takes in a `name` and a `weight`, which is used when combining log likelihoods.\n",
    "\n",
    "Importantly, each component takes in the calibration target, the real data that was observed, in an argument called `expected`. This argument should be a Pandas Dataframe with one row per time point and columns that will depend on the specific component type. For example, the `Binomial` component requires columns of `n` (trials) and `x` (successes).\n",
    "\n",
    "The components also handle extracting data from each simulation using the `extract_fn` argument. The value of this argument should be a function that takes in a simulation and returns a Pandas DataFrame. The specifics of the columns will depend a bit on the type of component (e.g. `BetaBinomial` is different from `Normal`), but often looks like a simulated version of `expected`. We will see examples below.\n",
    "\n",
    "We'll also see how to use the `conform` argument, the purpose of which is to temporally align the simulation output to the real data. This argument works along with the `extract_fn` to produce the final simulation outputs that are used in the likelihood function. The conformer is a function that takes in the `expected` data you provided and the `actual` simulation result the comes out of the `extract_fn`. The conformers we have built in are as follows:\n",
    "* `step_containing`: Conform by simply choosing the simulated timestep that contains the time indicated in the real data (`expected`)\n",
    "* `prevalent`: Interpolate the simulated timepoints to estimate the values that would have occurred at each real timepoint\n",
    "* `incident`: While the two methods above capture the state of the model at a particular point in time (stocks), this component allows you to capture the behavior of the model over time (flows). Instead of just giving one time value, `t`, you'll provide a second time value as well called `t1`. This conformer will add up events occurring between the two time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a Binomial component, as might be used to calibrate disease prevalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = ss.Normal(\n",
    "    name = 'Disease prevalence',\n",
    "    conform = 'prevalent',\n",
    "\n",
    "    expected = pd.DataFrame({\n",
    "        'x': [0.13, 0.16, 0.06],    # Prevalence of infection\n",
    "    }, index=pd.Index([ss.date(d) for d in ['2020-01-12', '2020-01-25', '2020-02-02']], name='t')), # On these dates\n",
    "    \n",
    "    extract_fn = lambda sim: pd.DataFrame({\n",
    "        'x': sim.results.sir.prevalence,\n",
    "    }, index=pd.Index(sim.results.timevec, name='t')),\n",
    "\n",
    "    # You can specify the variance as well, but it's optional (max likelihood estimates will be used if not provided)\n",
    "    # This could be a single float or an array with the same shape as the expected values\n",
    "    sigma2 = 0.05, # e.g. (num_replicates/sigma2_model + 1/sigma2_data)^-1\n",
    "    #sigma2 = np.array([0.05, 0.25, 0.01])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can bring all the pieces together. We make a single base simulation and create an instance of a Starsim Calibration object. This object requires a few arguments, like the `calib_pars` and `sim`. We also pass in the function that modifies the base `sim`, here our `build_sim` function. No additional `build_kw` are required in this example.\n",
    "\n",
    "We also pass in a list of `components`. Instead of using this \"component-based\" system, a user could simply provide an `eval_fn`, which takes in a completed sim an any `eval_kwargs` and returns a \"mismatch\" score to be minimized.\n",
    "\n",
    "We can also specify the total number of trials to run, the number of parallel works, and a few other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.heading('Beginning calibration')\n",
    "\n",
    "# Make the sim and data\n",
    "sim = make_sim()\n",
    "\n",
    "# Make the calibration\n",
    "calib = ss.Calibration(\n",
    "    calib_pars = calib_pars,\n",
    "    sim = sim,\n",
    "    build_fn = build_sim,\n",
    "    build_kw = dict(n_reps=3), # Run 3 replicates for each parameter set\n",
    "    reseed = True, # If true, a different random seed will be provided to each configuration\n",
    "    components = [prevalence],\n",
    "    total_trials = 100,\n",
    "    n_workers = None, # None indicates to use all available CPUs\n",
    "    die = True,\n",
    "    debug = debug, # Run in serial if True\n",
    ")\n",
    "\n",
    "# Perform the calibration\n",
    "sc.printcyan('\\nPeforming calibration...')\n",
    "calib.calibrate();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the best parameters that were found. Note that the `rand_seed` was selected at random, but the other parameters are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.best_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the calibration is complete, we can compare the `guess` values to the best values found by calling `check_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm - Note the comparison is here configured over n_reps=15 replicates\n",
    "sc.printcyan('\\nConfirming fit...')\n",
    "\n",
    "# Increase replicates to 15 for more representative results when running check_fit\n",
    "calib.build_kw['n_reps'] = 15\n",
    "\n",
    "calib.check_fit(do_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling `check_fit`, we can plot the results. This first plot shows the Normal likelihood distributions from each of the 15 simulations we did in `check_fit` as the colored lines. The vertical dashed line is located at the real (`expected`) data. Top row is the \"guess\" values and the bottom row is the new \"best\" parameters. We want the vertical dashed line to cross the Gaussians at high points, representing high likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to plot the results is via bootstrapping. Here we repeatedly choose 15 from the `n_reps=15` simulations (with replacement), compute the average (or sum for some components), and repeatedly calculate the mean. We then plot the distribution of means, and hope it lands near the vertical dashed lines representing the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.plot(bootstrap=True); # Pass bootstrap=True to produce this plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view some plots of the final fitted results. Whereas the two plots above were from the `check_fit`, running both \"guess\" and \"best\" parameters, here we make make new simulations to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = calib.plot_final(); # Run the model for build_kw['n_reps'] = 15 replicates\n",
    "for ax in g.axes: # Fix the date formatting\n",
    "    ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna has lots of diagnostic plots that we can explore. Possible plots include:\n",
    "* plot_contour\n",
    "* plot_edf\n",
    "* plot_hypervolume_history\n",
    "* plot_intermediate_values\n",
    "* plot_optimization_history\n",
    "* plot_parallel_coordinate\n",
    "* plot_param_importances\n",
    "* plot_pareto_front\n",
    "* plot_rank\n",
    "* plot_slice\n",
    "* plot_terminator_improvement\n",
    "* plot_timeline\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.plot_optuna('plot_optimization_history'); # Plot the optimization history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.plot_optuna('plot_contour');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.plot_optuna('plot_param_importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose not to use components, you can always create your own mismatch function, as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = (ss.date('2020-01-12'), 0.13)\n",
    "\n",
    "def eval(sim, expected):\n",
    "    # Compute the squared error at one point in time.\n",
    "    # expected will contain my_data in this example due to eval_kw\n",
    "    date, p = expected\n",
    "    if not isinstance(sim, ss.MultiSim):\n",
    "        sim = ss.MultiSim(sims=[sim])\n",
    "\n",
    "    ret = 0\n",
    "    for s in sim.sims:\n",
    "        ind = np.searchsorted(s.results.timevec, date, side='left')\n",
    "        prev = s.results.sir.prevalence[ind]\n",
    "        ret += (prev - p)**2\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Define the calibration parameters\n",
    "calib_pars = dict(\n",
    "    beta = dict(low=0.01, high=0.30, guess=0.15, suggest_type='suggest_float', log=True),\n",
    ")\n",
    "\n",
    "# Make the sim and data\n",
    "sim = make_sim()\n",
    "\n",
    "# Make the calibration\n",
    "calib = ss.Calibration(\n",
    "    calib_pars = calib_pars,\n",
    "    sim = sim,\n",
    "    build_fn = build_sim,\n",
    "    build_kw = dict(n_reps=2), # Two reps per point\n",
    "    reseed = True,\n",
    "    eval_fn = eval, # Will call my_function(msim, eval_kwargs)\n",
    "    eval_kw = dict(expected=my_data), # Will call eval(sim, **eval_kw)\n",
    "    total_trials = 10,\n",
    "    n_workers = None, # None indicates to use all available CPUs\n",
    "    die = True,\n",
    "    debug = debug,\n",
    ")\n",
    "\n",
    "# Perform the calibration\n",
    "sc.printcyan('\\nPeforming calibration...')\n",
    "calib.calibrate()\n",
    "\n",
    "# Check\n",
    "calib.check_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more, take a look at `test_calibration.py` in the `tests` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
